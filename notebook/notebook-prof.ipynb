{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 76"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T12:14:18.121498Z",
     "iopub.status.busy": "2025-10-02T12:14:18.120766Z",
     "iopub.status.idle": "2025-10-02T12:14:18.154408Z",
     "shell.execute_reply": "2025-10-02T12:14:18.153694Z",
     "shell.execute_reply.started": "2025-10-02T12:14:18.121473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kelas yang terdeteksi: ['dayak', 'minangkabau', 'balinese', 'javanese', 'batak']\n",
      "\n",
      "Memulai proses penggabungan data 'val' ke 'train'...\n",
      "Proses selesai. Sebanyak 0 file telah dipindahkan dari 'val' ke 'train'.\n",
      "Direktori './dataset-logika-resize/val' sekarang kosong dan bisa dihapus.\n",
      "\n",
      "Membaca '/kaggle/input/delete/delete_file.csv'. Terdapat 240 file untuk dihapus.\n",
      "Proses pembersihan selesai. Sebanyak 240 file telah dihapus dari direktori 'train'.\n",
      "\n",
      "Struktur data akhir siap digunakan:\n",
      "- Direktori Train: ./dataset-logika-resize/train\n",
      "- Direktori Test: ./dataset-logika-resize/test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Konfigurasi Path ---\n",
    "# Sesuaikan path ini dengan lokasi direktori dataset Anda\n",
    "base_path = './'\n",
    "train_dir = os.path.join(base_path, 'train')\n",
    "test_dir = os.path.join(base_path, 'test')\n",
    "\n",
    "# Verifikasi struktur folder\n",
    "print(f\"Train directory: {train_dir}\")\n",
    "print(f\"Test directory: {test_dir}\")\n",
    "print(f\"Train exists: {os.path.exists(train_dir)}\")\n",
    "print(f\"Test exists: {os.path.exists(test_dir)}\")\n",
    "\n",
    "# Mendapatkan daftar kelas dari folder train (5 kelas)\n",
    "if os.path.exists(train_dir):\n",
    "    classes = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
    "    print(f\"Kelas yang terdeteksi: {classes}\")\n",
    "    print(f\"Jumlah kelas: {len(classes)}\")\n",
    "    \n",
    "    # Tampilkan jumlah file di setiap kelas\n",
    "    for cls in classes:\n",
    "        class_path = os.path.join(train_dir, cls)\n",
    "        if os.path.exists(class_path):\n",
    "            file_count = len([f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            print(f\"  - {cls}: {file_count} gambar\")\n",
    "else:\n",
    "    print(\"Error: Folder train tidak ditemukan!\")\n",
    "    classes = []\n",
    "\n",
    "# --- 2. Verifikasi Struktur Data ---\n",
    "print(\"\\n=== VERIFIKASI STRUKTUR DATA ===\")\n",
    "print(\"Struktur data siap digunakan:\")\n",
    "print(f\"- Direktori Train: {train_dir}\")\n",
    "print(f\"- Direktori Test: {test_dir}\")\n",
    "\n",
    "if classes:\n",
    "    print(\"\\nJumlah gambar per kelas:\")\n",
    "    total_images = 0\n",
    "    for cls in classes:\n",
    "        class_path = os.path.join(train_dir, cls)\n",
    "        if os.path.exists(class_path):\n",
    "            file_count = len([f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            print(f\"  - {cls}: {file_count} gambar\")\n",
    "            total_images += file_count\n",
    "    \n",
    "    print(f\"\\nTotal gambar untuk training: {total_images}\")\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "import timm # Pustaka untuk model SOTA seperti ConvNeXt\n",
    "from tqdm.auto import tqdm\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torchmetrics.classification import MulticlassF1Score\n",
    "\n",
    "# --- Persiapan Awal (CUDA, Seed) ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# --- Point 5: Implementasi Focal Loss ---\n",
    "# PyTorch tidak memiliki Focal Loss bawaan, jadi kita definisikan sendiri.\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt)**self.gamma * ce_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# --- Point 1, 2, 3, 7: Definisi Augmentasi ---\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=224, scale=(0.8, 1.0)), # Point 1\n",
    "    transforms.RandomRotation(degrees=15), # Point 2\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5), # Point 2\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1), # Point 3\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.2)), # Point 7\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# --- Memuat Data ---\n",
    "DATA_DIR = './train'  # Sesuaikan dengan struktur folder Anda\n",
    "full_dataset = datasets.ImageFolder(DATA_DIR, transform=train_transforms)\n",
    "train_size = int(0.85 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_subset, val_subset_aug = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Ganti transform val_subset agar tidak ada augmentasi saat evaluasi\n",
    "val_subset_aug.dataset.transform = val_test_transforms\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_subset_aug, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "NUM_CLASSES = len(full_dataset.classes)\n",
    "\n",
    "print(f\"\\nDataset information:\")\n",
    "print(f\"Total images: {len(full_dataset)}\")\n",
    "print(f\"Training images: {len(train_subset)}\")\n",
    "print(f\"Validation images: {len(val_subset_aug)}\")\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")\n",
    "print(f\"Class names: {full_dataset.classes}\")\n",
    "\n",
    "# --- Point 4: Definisi Model ConvNeXt ---\n",
    "model = timm.create_model(\n",
    "    'convnext_tiny.in12k_ft_in1k', # Menggunakan varian ConvNeXt Tiny yang sudah di-fine-tune\n",
    "    pretrained=True,\n",
    "    num_classes=NUM_CLASSES\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# --- Point 6: Strategi Gradual Unfreezing & Differential Learning Rates ---\n",
    "\n",
    "# --- TAHAP 1: Latih hanya kepala (classifier) ---\n",
    "print(\"\\n--- TAHAP 1: Melatih Kepala Klasifikasi ---\")\n",
    "# Bekukan semua layer kecuali kepala\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.head.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.AdamW(model.head.parameters(), lr=1e-3)\n",
    "criterion = FocalLoss().to(device)\n",
    "f1_metric = MulticlassF1Score(num_classes=NUM_CLASSES, average='macro').to(device)\n",
    "\n",
    "# Loop pelatihan singkat untuk kepala\n",
    "for epoch in range(3): # Cukup 3 epoch untuk pemanasan\n",
    "    model.train()\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/3\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "print(\"Pelatihan kepala selesai.\\n\")\n",
    "\n",
    "# --- TAHAP 2: Fine-tuning seluruh model dengan Differential LR ---\n",
    "print(\"--- TAHAP 2: Fine-tuning Seluruh Model ---\")\n",
    "# Cairkan (unfreeze) semua layer\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Siapkan parameter group untuk Differential LR\n",
    "optimizer = optim.AdamW([\n",
    "    {'params': model.head.parameters(), 'lr': 1e-4}, # LR lebih tinggi untuk kepala\n",
    "    {'params': [p for n, p in model.named_parameters() if 'head' not in n], 'lr': 1e-5} # LR lebih rendah untuk backbone\n",
    "], weight_decay=1e-3)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_eta_min=1e-6, T_max=10)\n",
    "\n",
    "# --- Loop Pelatihan Utama ---\n",
    "NUM_EPOCHS = 15\n",
    "best_f1 = 0.0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}/{NUM_EPOCHS}\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "            all_preds.append(preds)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    # Hitung F1-score untuk epoch ini\n",
    "    val_f1 = f1_metric(torch.cat(all_preds), torch.cat(all_labels))\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}, Val F1-Score: {val_f1:.4f}\")\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # Simpan model jika F1-score membaik\n",
    "    if val_f1 > best_f1:\n",
    "        best_f1 = val_f1\n",
    "        torch.save(model.state_dict(), 'best_model_convnext.pth')\n",
    "        print(f\"Model terbaik disimpan dengan F1-score: {best_f1:.4f}\")\n",
    "\n",
    "print(\"\\nPelatihan selesai!\")\n",
    "print(f\"F1-score terbaik di set validasi: {best_f1:.4f}\")\n",
    "\n",
    "# --- Point 8: Fungsi untuk Test Time Augmentation (TTA) ---\n",
    "def predict_with_tta(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_final_preds = []\n",
    "    \n",
    "    # Definisikan augmentasi untuk TTA\n",
    "    tta_transforms = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(p=1.0), # Pasti di-flip\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    ])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(dataloader, desc=\"Predicting with TTA\"):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            # 1. Prediksi original\n",
    "            original_probs = F.softmax(model(images), dim=1)\n",
    "            \n",
    "            # 2. Prediksi dengan augmentasi (misal: flip)\n",
    "            flipped_images = tta_transforms(images)\n",
    "            flipped_probs = F.softmax(model(flipped_images), dim=1)\n",
    "            \n",
    "            # Rata-ratakan probabilitas\n",
    "            avg_probs = (original_probs + flipped_probs) / 2.0\n",
    "            final_preds = torch.argmax(avg_probs, dim=1)\n",
    "            all_final_preds.append(final_preds.cpu().numpy())\n",
    "            \n",
    "    return np.concatenate(all_final_preds)\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "# Pastikan semua fungsi dan variabel dari skrip training sebelumnya sudah didefinisikan\n",
    "# (model, FocalLoss, predict_with_tta, val_test_transforms, device, dll.)\n",
    "\n",
    "# --- 1. Muat Model Terbaik yang Telah Disimpan ---\n",
    "# Pastikan model sudah didefinisikan seperti di skrip training\n",
    "model.load_state_dict(torch.load('best_model_convnext.pth', map_location=device))\n",
    "print(\"Model terbaik (best_model_convnext.pth) berhasil dimuat.\")\n",
    "\n",
    "# --- 2. Siapkan DataLoader untuk Data Tes ---\n",
    "# Arahkan ke folder test yang sebenarnya\n",
    "TEST_DIR = './test/' \n",
    "\n",
    "# Kita gunakan ImageFolder agar mudah mendapatkan nama file dan kelas dummy\n",
    "# Gunakan transform yang sama dengan validasi\n",
    "test_dataset = ImageFolder(root=TEST_DIR, transform=val_test_transforms)\n",
    "\n",
    "# PENTING: shuffle=False agar urutan file tidak berubah\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Data tes dimuat dari: {TEST_DIR}\")\n",
    "print(f\"Jumlah gambar tes: {len(test_dataset)}\\n\")\n",
    "\n",
    "# --- 3. Jalankan Prediksi dengan TTA ---\n",
    "predictions_indices = predict_with_tta(model, test_loader, device)\n",
    "print(f\"Prediksi dengan TTA selesai. Total prediksi: {len(predictions_indices)}\")\n",
    "\n",
    "# --- 4. Format Output ke dalam CSV ---\n",
    "# Dapatkan nama kelas dari dataset yang kita latih\n",
    "class_names = full_dataset.classes # 'full_dataset' dari skrip training\n",
    "# -> ['balinese', 'batak', 'dayak', 'javanese', 'minangkabau']\n",
    "\n",
    "# Dapatkan nama file asli dari test_dataset\n",
    "# test_dataset.samples berisi path lengkap, kita hanya butuh nama filenya\n",
    "test_filenames = [os.path.basename(path) for path, _ in test_dataset.samples]\n",
    "\n",
    "# Hapus ekstensi file (misal: .jpg) dari nama file untuk mendapatkan ID\n",
    "test_ids = [os.path.splitext(name)[0] for name in test_filenames]\n",
    "\n",
    "# Ubah indeks prediksi menjadi nama kelas\n",
    "predicted_styles = [class_names[i] for i in predictions_indices]\n",
    "\n",
    "# Buat DataFrame pandas\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'style': predicted_styles\n",
    "})\n",
    "\n",
    "# Simpan ke file CSV\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"\\nFile 'submission.csv' telah berhasil dibuat dan siap diunggah!\")\n",
    "print(\"Contoh isi file:\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8315643,
     "sourceId": 13126780,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8390057,
     "sourceId": 13241166,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8390261,
     "sourceId": 13241442,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
